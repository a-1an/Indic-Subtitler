{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7a3d9b-841f-4ed0-8ccc-377bc921f012",
   "metadata": {},
   "source": [
    "# GPT probabilities Experimentation\n",
    "\n",
    "Goal: To improve ASR transcription further with context\n",
    "\n",
    "Context can be like: Summary of youtube vide, important text terms mentioned etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b0a84-8710-46f7-a028-302271503992",
   "metadata": {},
   "source": [
    "example video=>\n",
    "\n",
    "https://www.youtube.com/watch?v=gNtu_1otRiw\n",
    "\n",
    "Context:\n",
    "\n",
    "This is a youtube video Shashi tharoor and Thiagarajan discuss about why Centre is not giving equal amount of taxes to South Indian states which is affecting them a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea49b6c7-7492-4269-b89a-36fadd1fe8fa",
   "metadata": {},
   "source": [
    "**Transcript of the video**\n",
    "\n",
    "for every one rupee of tax contributed by Uttar Pradesh. That state receives one rupees seventy nine paise back. From Central Tax, we won't get one rupee and we'll get thirty to thirty-five paise so it's not the money that's going to be bigger. It's the lack of progress. It's like throwing money down a well. One thing that's obviously clear. Oh, my God. The strike home to me is when you have it. Finance Commission for the first time.\n",
    "This is the last the Finance Commission is to take into account.\n",
    "The senses of twenty eleven. Rather than the census of 1971\n",
    "in determining allocations. How does that affect your stage and my stage?\n",
    "Just like that. The redistribution of MP seats has been frozen since nineteen.\n",
    "seventy-six Precisely because you didn't want to penalize those states,\n",
    "But if the goal of net transfers is equality or leveling of outcomes, then our financial commissions have been failing spectacularly. We have a situation where Tamil Nadu has a state budget that reflects only a small percentage of its tax revenues. Even though its economy is smaller than Tamil Nadu's, it obviously doesn't put a strain on the federal government because the growing perception in the south seems to be that we are getting the end of the financial deal. In the short term, it's even more problematic in the long term. If you take Tamil Nadu, for example, at one point we were 7.5% of the country's economy. We were 7.5 percent of the population.\n",
    "And our share of the devolution of the horizontal between the states was 7.7%. In 20-25 years, we've gone down to under 6% of the population.\n",
    "So in the long run, if we go down this path, in another 15 or 20 years, we'll be 14 or 15 percent of GDP, and we'll get back two percent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac891267-9761-4ed0-9c94-8e208fce44ff",
   "metadata": {},
   "source": [
    "## Ideas discussed\n",
    "\n",
    "I was also thinking of something like that \n",
    "Basically getting the probabilities of every word in the sentence then changing low probability words to blanks and asking LLM s to fill that\n",
    "\n",
    "https://stackoverflow.com/questions/76397904/generate-the-probabilities-of-all-the-next-possible-word-for-a-given-text \n",
    "\n",
    "Which means you can create chunks for fixing but feed full previous context \n",
    "\n",
    "Example\n",
    "Doc of 100 sentences \n",
    "Chunked on 20 sentences \n",
    "\n",
    "Chunk 1 sees 20 sentences \n",
    "\n",
    "Chunk 2 you are fixing next 20 but you show even the previous chunk for context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cceae7-eba8-45dd-8af1-e8c3e5361513",
   "metadata": {},
   "source": [
    "## According to Simrat\n",
    "\n",
    "Hey this is exactly what you need \n",
    "https://huggingface.co/spaces/joaogante/color-coded-text-generation/blob/main/app.py \n",
    "\n",
    "That’s the first part of identifying incorrect tokens \n",
    "You can have another next token prediction output \n",
    "\n",
    "Zip the two \n",
    "\n",
    "Whenever you find a low probability token you can fix it by taking the output of the model \n",
    "\n",
    "You can also provide context to next token generation query so that token predictions are more aligned \n",
    "Especially for starting sentences \n",
    "\n",
    "My thought is that you can have a summary at the start that is like bootstrapping context even for the first sentence as GPT is not bi directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afd48d-b93e-4b01-93c4-8908128771e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdb36a-f250-431a-bc19-c98d8bfac622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
